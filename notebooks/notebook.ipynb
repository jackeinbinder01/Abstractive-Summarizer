{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import CNN/Daily Mail Dataset",
   "id": "b4230ae7eac6c837"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T05:08:48.535020Z",
     "start_time": "2025-08-10T05:08:46.933827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from utils.sample import sample, print_sample\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import set_seed\n",
    "\n",
    "# Seed for reproducability\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "corpus = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "train_corpus = corpus[\"train\"]\n",
    "test_corpus = corpus[\"test\"]\n",
    "\n",
    "example = sample(train_corpus)\n",
    "print_sample(example, max_chars=200)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "\"(CNN) -- A magnitude 7.9 earthquake struck off the coast of central Peru on Wednesday evening, killing 15 people and leaving 70 hurt, President Alan Garcia said on national television. Pedestrians tr...\n",
      "\n",
      "Highlights:\n",
      "\"NEW: Tsunami warnings and watches canceled, as is Hawaii's advisory .\\nAt least 15 people killed, 70 injured in quake .\\nQuake was felt for two minutes; people ran out of office buildings in panic .\\...\n",
      "\n",
      "Id:\n",
      "\"fca2c956e45391f8b205cbe78e6b454321d49305\"\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare BART and PEGASUS models",
   "id": "14269a1c26f96ea4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Select Samples",
   "id": "e9f8b8f33b80a741"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T05:08:48.540233Z",
     "start_time": "2025-08-10T05:08:48.536038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select 100 articles\n",
    "NUM_SAMPLES = 100\n",
    "articles = sample(test_corpus, NUM_SAMPLES)"
   ],
   "id": "8bcbd4a9b7b0e33",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Models",
   "id": "dd221376c4a157f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T05:08:52.125949Z",
     "start_time": "2025-08-10T05:08:48.540785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.pipeline import make_bart_pipeline, make_pegasus_pipeline\n",
    "\n",
    "bart_pipeline = make_bart_pipeline()\n",
    "pegasus_pipeline = make_pegasus_pipeline()"
   ],
   "id": "e93ab5a0595e8032",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare Summarizers",
   "id": "c72e8b945e808045"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bdbf5227fc57b1c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T05:08:52.128599Z",
     "start_time": "2025-08-10T05:08:52.126977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.summarizer import Summarizer\n",
    "\n",
    "model_configs = {\n",
    "    'bart': Summarizer(bart_pipeline),\n",
    "    'pegasus': Summarizer(pegasus_pipeline)\n",
    "}"
   ],
   "id": "7120eeb58417d376",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "517b4b336cfd226e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate & Evaluate Summaries",
   "id": "7e2687f69f95f406"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T05:09:59.342458Z",
     "start_time": "2025-08-10T05:08:52.129026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.sum_eval import sum_eval\n",
    "from utils.plot import plot_avg_rouge, plot_avg_manual_score\n",
    "import pandas as pd\n",
    "\n",
    "results_dir = os.path.join(\"..\", \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "model_scores = sum_eval(articles, model_configs)\n",
    "df_models = pd.DataFrame(model_scores).T.sort_values(by='rougeL', ascending=False)\n",
    "\n",
    "plot_avg_rouge(model_scores, save_path=os.path.join(results_dir, 'avg_rouge_scores'))"
   ],
   "id": "7f4fe1a23c4a8eed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: BART...\n",
      "\n",
      "Generating summaries for 100 articles...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m results_dir = os.path.join(\u001B[33m\"\u001B[39m\u001B[33m..\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mresults\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m os.makedirs(results_dir, exist_ok=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m model_scores = \u001B[43msum_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43marticles\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_configs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m df_models = pd.DataFrame(model_scores).T.sort_values(by=\u001B[33m'\u001B[39m\u001B[33mrougeL\u001B[39m\u001B[33m'\u001B[39m, ascending=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m     11\u001B[39m plot_avg_rouge(model_scores, save_path=os.path.join(results_dir, \u001B[33m'\u001B[39m\u001B[33mavg_rouge_scores\u001B[39m\u001B[33m'\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/abstractive_summarizer/utils/sum_eval.py:15\u001B[39m, in \u001B[36msum_eval\u001B[39m\u001B[34m(corpus, configs)\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEvaluating: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel.upper()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     14\u001B[39m local_corpus = deepcopy(corpus)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m results = \u001B[43msummarizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstructured_batch_summarize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_corpus\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# Defensive filtering of malformed entries\u001B[39;00m\n\u001B[32m     18\u001B[39m valid = [\n\u001B[32m     19\u001B[39m     doc \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[32m     20\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc.get(\u001B[33m\"\u001B[39m\u001B[33msummary\u001B[39m\u001B[33m\"\u001B[39m), \u001B[38;5;28mstr\u001B[39m)\n\u001B[32m     21\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc.get(\u001B[33m\"\u001B[39m\u001B[33mhighlights\u001B[39m\u001B[33m\"\u001B[39m), \u001B[38;5;28mstr\u001B[39m)\n\u001B[32m     22\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m doc[\u001B[33m\"\u001B[39m\u001B[33msummary\u001B[39m\u001B[33m\"\u001B[39m].strip() \u001B[38;5;129;01mand\u001B[39;00m doc[\u001B[33m\"\u001B[39m\u001B[33mhighlights\u001B[39m\u001B[33m\"\u001B[39m].strip()\n\u001B[32m     23\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/abstractive_summarizer/core/summarizer.py:32\u001B[39m, in \u001B[36mstructured_batch_summarize\u001B[39m\u001B[34m(self, corpus, max_articles)\u001B[39m\n\u001B[32m     28\u001B[39m     document = corpus[i]\n\u001B[32m     29\u001B[39m     summary = \u001B[38;5;28mself\u001B[39m.summarize(document[\u001B[33m\"\u001B[39m\u001B[33marticle\u001B[39m\u001B[33m\"\u001B[39m], **kwargs)\n\u001B[32m     30\u001B[39m     documents.append({\n\u001B[32m     31\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33marticle\u001B[39m\u001B[33m\"\u001B[39m: document[\u001B[33m\"\u001B[39m\u001B[33marticle\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mhighlights\u001B[39m\u001B[33m\"\u001B[39m: document[\u001B[33m\"\u001B[39m\u001B[33mhighlights\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     33\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33msummary\u001B[39m\u001B[33m\"\u001B[39m: summary,\n\u001B[32m     34\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m: document[\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     35\u001B[39m     })\n\u001B[32m     37\u001B[39m end = time.perf_counter()\n\u001B[32m     38\u001B[39m elapsed = end - start\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/abstractive_summarizer/core/summarizer.py:10\u001B[39m, in \u001B[36msummarize\u001B[39m\u001B[34m(self, article, min_length, max_length, **kwargs)\u001B[39m\n\u001B[32m      7\u001B[39m     \u001B[38;5;28mself\u001B[39m.model = model\n\u001B[32m      8\u001B[39m     \u001B[38;5;28mself\u001B[39m.gen_defaults = gen_defaults\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msummarize\u001B[39m(\u001B[38;5;28mself\u001B[39m, article: \u001B[38;5;28mstr\u001B[39m, **kwargs) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m     11\u001B[39m     params = {**\u001B[38;5;28mself\u001B[39m.gen_defaults, **kwargs}\n\u001B[32m     12\u001B[39m     out = \u001B[38;5;28mself\u001B[39m.model(article, **params)[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33msummary_text\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:303\u001B[39m, in \u001B[36mSummarizationPipeline.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    279\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m    280\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    281\u001B[39m \u001B[33;03m    Summarize the text(s) given as inputs.\u001B[39;00m\n\u001B[32m    282\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    301\u001B[39m \u001B[33;03m          ids of the summary.\u001B[39;00m\n\u001B[32m    302\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m303\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:191\u001B[39m, in \u001B[36mText2TextGenerationPipeline.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    162\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]], **kwargs: Any) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]]:\n\u001B[32m    163\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    164\u001B[39m \u001B[33;03m    Generate the output text(s) using text(s) given as inputs.\u001B[39;00m\n\u001B[32m    165\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    188\u001B[39m \u001B[33;03m          ids of the generated text.\u001B[39;00m\n\u001B[32m    189\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m     result = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    192\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    193\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(args[\u001B[32m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m)\n\u001B[32m    194\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(el, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m el \u001B[38;5;129;01min\u001B[39;00m args[\u001B[32m0\u001B[39m])\n\u001B[32m    195\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mlen\u001B[39m(res) == \u001B[32m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m result)\n\u001B[32m    196\u001B[39m     ):\n\u001B[32m    197\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m [res[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m result]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/base.py:1458\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1450\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1451\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1452\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1455\u001B[39m         )\n\u001B[32m   1456\u001B[39m     )\n\u001B[32m   1457\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1458\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/base.py:1465\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1463\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[32m   1464\u001B[39m     model_inputs = \u001B[38;5;28mself\u001B[39m.preprocess(inputs, **preprocess_params)\n\u001B[32m-> \u001B[39m\u001B[32m1465\u001B[39m     model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1466\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m.postprocess(model_outputs, **postprocess_params)\n\u001B[32m   1467\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/base.py:1365\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1363\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1364\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1365\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1366\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1367\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:220\u001B[39m, in \u001B[36mText2TextGenerationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, **generate_kwargs)\u001B[39m\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[32m    218\u001B[39m     generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.generation_config\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m output_ids = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m out_b = output_ids.shape[\u001B[32m0\u001B[39m]\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/generation/utils.py:2652\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2645\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2646\u001B[39m         input_ids=input_ids,\n\u001B[32m   2647\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2648\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2649\u001B[39m         **model_kwargs,\n\u001B[32m   2650\u001B[39m     )\n\u001B[32m   2651\u001B[39m     \u001B[38;5;66;03m# 12. run beam sample\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2652\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2653\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2654\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2655\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2656\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2657\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2658\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2659\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2661\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001B[32m   2662\u001B[39m     logger.warning_once(\n\u001B[32m   2663\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2664\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2665\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/generation/utils.py:4223\u001B[39m, in \u001B[36mGenerationMixin._beam_search\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[39m\n\u001B[32m   4211\u001B[39m     cur_len = cur_len + \u001B[32m1\u001B[39m\n\u001B[32m   4212\u001B[39m     is_early_stop_heuristic_unsatisfied = \u001B[38;5;28mself\u001B[39m._check_early_stop_heuristic(\n\u001B[32m   4213\u001B[39m         is_early_stop_heuristic_unsatisfied=is_early_stop_heuristic_unsatisfied,\n\u001B[32m   4214\u001B[39m         running_beam_scores=running_beam_scores,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4221\u001B[39m         length_penalty=length_penalty,\n\u001B[32m   4222\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m4223\u001B[39m     this_peer_finished = \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_beam_search_has_unfinished_sequences\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4224\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_early_stop_heuristic_unsatisfied\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4225\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_sent_finished\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4226\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnext_token_hits_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4227\u001B[39m \u001B[43m        \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4228\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4230\u001B[39m \u001B[38;5;66;03m# 5. prepare outputs\u001B[39;00m\n\u001B[32m   4231\u001B[39m \u001B[38;5;66;03m# Take best beams for each batch (the score is sorted in descending order)\u001B[39;00m\n\u001B[32m   4232\u001B[39m sequences = \u001B[38;5;28mself\u001B[39m._flatten_beam_dim(sequences[:, :num_return_sequences, :])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/generation/utils.py:3787\u001B[39m, in \u001B[36mGenerationMixin._beam_search_has_unfinished_sequences\u001B[39m\u001B[34m(is_early_stop_heuristic_unsatisfied, is_sent_finished, next_token_hits_stopping_criteria, early_stopping)\u001B[39m\n\u001B[32m   3782\u001B[39m     worst_finished_score = torch.where(is_sent_finished, torch.min(beam_scores, dim=\u001B[32m1\u001B[39m, keepdim=\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[32m0\u001B[39m], -\u001B[32m1.0e9\u001B[39m)\n\u001B[32m   3783\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m is_early_stop_heuristic_unsatisfied & torch.any(\n\u001B[32m   3784\u001B[39m         best_possible_running_score > worst_finished_score, dim=-\u001B[32m1\u001B[39m, keepdim=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   3785\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m3787\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m   3788\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_beam_search_has_unfinished_sequences\u001B[39m(\n\u001B[32m   3789\u001B[39m     is_early_stop_heuristic_unsatisfied: torch.Tensor,\n\u001B[32m   3790\u001B[39m     is_sent_finished: torch.Tensor,\n\u001B[32m   3791\u001B[39m     next_token_hits_stopping_criteria: torch.Tensor,\n\u001B[32m   3792\u001B[39m     early_stopping: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   3793\u001B[39m ):\n\u001B[32m   3794\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   3795\u001B[39m \u001B[33;03m    Beam Search stopping condition -- halts the generation loop if any of these conditions becomes False\u001B[39;00m\n\u001B[32m   3796\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   3797\u001B[39m     \u001B[38;5;66;03m# a. Can the open beams improve the top completed scores?\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extrinsic Evaluation",
   "id": "4c4d1def6ee0ec64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select 10 articles\n",
    "NUM_RANDOM_SAMPLES = 10\n",
    "manual_articles = sample(test_corpus, NUM_RANDOM_SAMPLES)"
   ],
   "id": "873b63420f01dd19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate summaries\n",
    "bart_summarizer = model_configs['bart']\n",
    "bart_summaries = bart_summarizer.structured_batch_summarize(manual_articles)"
   ],
   "id": "f631cd45eb986ee5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART",
   "id": "715c512aba2d9351"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from evaluation.extrinsic_evaluator import ExtrinsicEvaluator\n",
    "\n",
    "# Evaluate Bart\n",
    "bart_evaluator = ExtrinsicEvaluator(bart_summaries)\n",
    "bart_evaluator.evaluate('BART')\n",
    "bart_id_to_score = {\n",
    "    \"432642e19523c71e986985b0a1bb82f6baa9dc4f\": 5,\n",
    "    \"15fffd5f6e0249a50b8ff64afa785a56e3cd6329\": 5,\n",
    "    \"a4f3403c0c05589f939d0f4ddb138e8e2c5d288e\": 5,\n",
    "    \"bc526b7c50b3b816751a6a692cdb7324d980a576\": 5,\n",
    "    \"ca5cf6768d5700c8e944d7ec773d2116075d9003\": 5,\n",
    "    \"41722a8d14392fd2999bafbcec86f04d5634590f\": 5,\n",
    "    \"85afff41a6d8d564588c54c6b5b9ff972dfe0eff\": 5,\n",
    "    \"868be1a9da96d8ddbc357599943293a7684eb271\": 5,\n",
    "    \"197354baef143a117e28c3d7e6497fe3f08c975f\": 5,\n",
    "    \"f9b726faba76c1808f033491ef21960438ce5914\": 5,\n",
    "}\n",
    "bart_evaluator.submit_scores(bart_id_to_score)\n"
   ],
   "id": "230350b2fe91e643",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### PEGASUS",
   "id": "16cae304943bd622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "pegasus_summarizer = model_configs['pegasus']\n",
    "pegasus_summaries = pegasus_summarizer.structured_batch_summarize(manual_articles)"
   ],
   "id": "20ff713cc035e9d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate PEGASUS\n",
    "pegasus_evaluator = ExtrinsicEvaluator(pegasus_summaries)\n",
    "pegasus_evaluator.evaluate('PEGASUS')\n",
    "pegasus_id_to_score = {\n",
    "    \"432642e19523c71e986985b0a1bb82f6baa9dc4f\": 5,\n",
    "    \"15fffd5f6e0249a50b8ff64afa785a56e3cd6329\": 5,\n",
    "    \"a4f3403c0c05589f939d0f4ddb138e8e2c5d288e\": 5,\n",
    "    \"bc526b7c50b3b816751a6a692cdb7324d980a576\": 5,\n",
    "    \"ca5cf6768d5700c8e944d7ec773d2116075d9003\": 5,\n",
    "    \"41722a8d14392fd2999bafbcec86f04d5634590f\": 5,\n",
    "    \"85afff41a6d8d564588c54c6b5b9ff972dfe0eff\": 5,\n",
    "    \"868be1a9da96d8ddbc357599943293a7684eb271\": 5,\n",
    "    \"197354baef143a117e28c3d7e6497fe3f08c975f\": 5,\n",
    "    \"f9b726faba76c1808f033491ef21960438ce5914\": 5,\n",
    "}\n",
    "pegasus_evaluator.submit_scores(pegasus_id_to_score)"
   ],
   "id": "51461789296750ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Manual Scores",
   "id": "97f821a9f9a00879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bart_evaluator.plot('BART', save_path=os.path.join(results_dir, 'bart'))\n",
    "pegasus_evaluator.plot('PEGASUS', save_path=os.path.join(results_dir, 'pegasus'))\n",
    "\n",
    "avg_manual_scores = {\n",
    "    'bart': bart_evaluator.avg_score,\n",
    "    'pegasus': pegasus_evaluator.avg_score\n",
    "}\n",
    "\n",
    "plot_avg_manual_score(avg_manual_scores, save_path=os.path.join(results_dir, 'avg_manual_scores'))"
   ],
   "id": "85834eeb3af3d67b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Winner is BART",
   "id": "acbd5df165171b3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ablation Study",
   "id": "2c0cd1aa345c76d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyperparameter Strategies",
   "id": "e18908024e1f26fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.pipeline import make_bart_pipeline\n",
    "\n",
    "DEFAULT_MIN_LENGTH = 30\n",
    "DEFAULT_MAX_LENGTH = 100\n",
    "\n",
    "bart_pipeline = make_bart_pipeline()\n",
    "\n",
    "ablation_configs = {\n",
    "    'bart_default': Summarizer(\n",
    "        bart_pipeline,\n",
    "        min_length= DEFAULT_MIN_LENGTH,\n",
    "        max_length= DEFAULT_MAX_LENGTH\n",
    "    ),\n",
    "    'bart_beam_light': Summarizer(\n",
    "        bart_pipeline,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        min_length= DEFAULT_MIN_LENGTH,\n",
    "        max_length= DEFAULT_MAX_LENGTH\n",
    "    ),\n",
    "    'bart_beam': Summarizer(\n",
    "        bart_pipeline,\n",
    "        num_beams=8,\n",
    "        early_stopping=True,\n",
    "        min_length= DEFAULT_MIN_LENGTH,\n",
    "        max_length= DEFAULT_MAX_LENGTH\n",
    "    ),\n",
    "    'bart_short': Summarizer(\n",
    "        bart_pipeline,\n",
    "        min_length=20,  # shorter min-length\n",
    "        max_length=60  # shorter max-length\n",
    "    ),\n",
    "    'bart_sample_basic': Summarizer(\n",
    "        bart_pipeline,\n",
    "        do_sample=True,\n",
    "        min_length= DEFAULT_MIN_LENGTH,\n",
    "        max_length= DEFAULT_MAX_LENGTH\n",
    "    ),\n",
    "    'bart_sample_temp': Summarizer(\n",
    "        bart_pipeline,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        min_length= DEFAULT_MIN_LENGTH,\n",
    "        max_length= DEFAULT_MAX_LENGTH\n",
    "    ),\n",
    "    'bart_sample_topk': Summarizer(\n",
    "        bart_pipeline,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=0.8,\n",
    "        min_length= DEFAULT_MIN_LENGTH,\n",
    "        max_length= DEFAULT_MAX_LENGTH\n",
    "    ),\n",
    "    'bart_sample_topp': Summarizer(\n",
    "        bart_pipeline,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        temperature=0.8,\n",
    "        min_length= DEFAULT_MIN_LENGTH,\n",
    "        max_length= DEFAULT_MAX_LENGTH\n",
    "    ),\n",
    "    'bart_sample_topkp': Summarizer(\n",
    "        bart_pipeline,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.92,\n",
    "        temperature=0.8,\n",
    "        min_length= DEFAULT_MIN_LENGTH,\n",
    "        max_length= DEFAULT_MAX_LENGTH\n",
    "    ),\n",
    "}"
   ],
   "id": "952c1d835c1ce4f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare Summarizers",
   "id": "ff8df2e056a0ba74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate & Evaluate Summaries",
   "id": "3b9985a88eb53448"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ablation_scores = sum_eval(articles, ablation_configs)\n",
    "df_ablation = pd.DataFrame(ablation_scores).T.sort_values(by='rougeL', ascending=False)\n",
    "plot_avg_rouge(ablation_scores, save_path=os.path.join(results_dir, 'ablation_avg_rouge_scores'))"
   ],
   "id": "a3e9797d25c7ff8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1886cf2fead13bff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Manual Evaluation\n",
    "\n"
   ],
   "id": "2ed3ca253887c1e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "11bfb57880f9d7c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select 3 random articles\n",
    "NUM_RANDOM_SAMPLES = 3\n",
    "manual_ablation_articles = sample(test_corpus, NUM_RANDOM_SAMPLES)"
   ],
   "id": "d99eb3cc8ab7b316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART DEFAULT",
   "id": "f384acc44191cb4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bart_default_summarizer = ablation_configs['bart_default']\n",
    "bart_default_summaries = bart_default_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "5977a291647603",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Default\n",
    "bart_default_evaluator = ExtrinsicEvaluator(bart_default_summaries)\n",
    "bart_default_evaluator.evaluate('BART DEFAULT')\n",
    "bart_default_id_to_score = {\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 5,\n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 5,\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 5\n",
    "}\n",
    "bart_default_evaluator.submit_scores(bart_default_id_to_score)"
   ],
   "id": "ecf2b7a4e685aeb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART BEAM LIGHT",
   "id": "869f7af88219ee8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "bart_beam_light_summarizer = ablation_configs['bart_beam_light']\n",
    "bart_beam_light_summaries = bart_beam_light_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "e33b90b4b86889ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Light Beam\n",
    "bart_beam_light_evaluator = ExtrinsicEvaluator(bart_beam_light_summaries)\n",
    "bart_beam_light_evaluator.evaluate('BART LIGHT BEAM')\n",
    "bart_beam_light_id_to_score = {\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 5,\n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 5,\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 5\n",
    "}\n",
    "bart_beam_light_evaluator.submit_scores(bart_beam_light_id_to_score)"
   ],
   "id": "b90a44515c562043",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART BEAM",
   "id": "a853ca5c7d3acf75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "bart_beam_summarizer = ablation_configs['bart_beam']\n",
    "bart_beam_summaries = bart_beam_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "4c314cb6f4851a52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Beam\n",
    "bart_beam_evaluator = ExtrinsicEvaluator(bart_beam_summaries)\n",
    "bart_beam_evaluator.evaluate('BART BEAM')\n",
    "bart_beam_id_to_score = {  # Summaries identical to bart beam light\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 5,  # Identical to bart beam light\n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 5,  # Identical to bart beam light\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 5  # Identical to bart beam light\n",
    "} \n",
    "bart_beam_evaluator.submit_scores(bart_beam_id_to_score)"
   ],
   "id": "315e0e7dedb4b1ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9958360d7b1a1460"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART SHORT",
   "id": "aed3a3332c497132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "bart_short_summarizer = ablation_configs['bart_short']\n",
    "bart_short_summaries = bart_short_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "344e7d12df6ca61a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Short\n",
    "bart_short_evaluator = ExtrinsicEvaluator(bart_short_summaries)\n",
    "bart_short_evaluator.evaluate('BART SHORT')\n",
    "bart_short_id_to_score = {  # Identical summaries to bart beam light and bart beam\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 5,\n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 5,\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 5\n",
    "}\n",
    "bart_short_evaluator.submit_scores(bart_short_id_to_score)"
   ],
   "id": "1fb7804f6c03324f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART SAMPLE BASIC",
   "id": "ed36afb61a018a61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "bart_sample_basic_summarizer = ablation_configs['bart_sample_basic']\n",
    "bart_sample_basic_summaries = bart_sample_basic_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "f0ed394d5ca1c92f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Sample Basic\n",
    "# These were almost the same as previous ones, but were slightly shorter and lost some important details\n",
    "\n",
    "bart_sample_basic_evaluator = ExtrinsicEvaluator(bart_sample_basic_summaries)\n",
    "bart_sample_basic_evaluator.evaluate('BART SAMPLE BASIC')\n",
    "bart_sample_basic_id_to_score = {  # First different summaries\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 4,  # Almost better than highlights. Doesnt mention loss to Manchester United\n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 4,  # Almost identical to previous, but doesn't mention Penny was cleared\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 4   # Doesnt mention talks with Aston Villa\n",
    "}\n",
    "bart_sample_basic_evaluator.submit_scores(bart_sample_basic_id_to_score)"
   ],
   "id": "b6e276421d3eeff3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART SAMPLE TEMP",
   "id": "5aa4a89c710485b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "bart_sample_temp_summarizer = ablation_configs['bart_sample_temp']\n",
    "bart_sample_temp_summaries = bart_sample_temp_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "b1971a5ffb9f6c40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Sample Basic\n",
    "bart_sample_temp_evaluator = ExtrinsicEvaluator(bart_sample_temp_summaries)\n",
    "bart_sample_temp_evaluator.evaluate('BART SAMPLE TEMP')\n",
    "bart_sample_temp_id_to_score = {\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 3,  # Doesn't mention loss to Manchester United, doesnt finish last thought\n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 3,  # Missing details on being cleared. Doesnt finish thought\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 4  # Just doesnt mention Aston Villa talks\n",
    "}\n",
    "bart_sample_temp_evaluator.submit_scores(bart_sample_temp_id_to_score)\n",
    "\n",
    "# Not as good as other models. Doesn't always finish sentences. Some details provided are not important"
   ],
   "id": "fb5cf143bad2a569",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART SAMPLE TOPK",
   "id": "9f90d165f1699fe8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "bart_sample_topk_summarizer = ablation_configs['bart_sample_topk']\n",
    "bart_sample_topk_summaries = bart_sample_topk_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "da8373679df12323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Sample TopK\n",
    "bart_sample_topk_evaluator = ExtrinsicEvaluator(bart_sample_topk_summaries)\n",
    "bart_sample_topk_evaluator.evaluate('BART SAMPLE TOPK')\n",
    "bart_sample_topk_id_to_score = {\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 3,  # Doesn't mention 4-2 loss to Manchester United \n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 5,  # Identical to bart light beam and bart beam\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 5  # Identical to bart light beam and bart beam\n",
    "}\n",
    "bart_sample_topk_evaluator.submit_scores(bart_sample_topk_id_to_score)\n",
    "\n",
    "# Almost as good as other models with some details being lost and a sentence not being completed"
   ],
   "id": "2a0874f72831d03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART SAMPLE TOPP",
   "id": "484a6ec1ab447300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "bart_sample_topp_summarizer = ablation_configs['bart_sample_topp']\n",
    "bart_sample_topp_summaries = bart_sample_topp_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "b80b1739f7b13e76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Sample TopP\n",
    "bart_sample_topp_evaluator = ExtrinsicEvaluator(bart_sample_topp_summaries)\n",
    "bart_sample_topp_evaluator.evaluate('BART SAMPLE TOPP')\n",
    "bart_sample_topp_id_to_score = {\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 3,  # Doesn't mention 4-2 loss to Manchester United\n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 5,  # Identical to previous summaries\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 4  # Doesn't mention some enquiries\n",
    "}\n",
    "bart_sample_topp_evaluator.submit_scores(bart_sample_topp_id_to_score)\n",
    "\n",
    "# Strong performance with some smaller details missing"
   ],
   "id": "5a0d5d1234e44089",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### BART SAMPLE TOPKP",
   "id": "74a2dd3c8abc5f9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate Summaries\n",
    "bart_sample_topkp_summarizer = ablation_configs['bart_sample_topkp']\n",
    "bart_sample_topkp_summaries = bart_sample_topkp_summarizer.structured_batch_summarize(manual_ablation_articles)"
   ],
   "id": "5787da74e3cc09b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate Bart Sample Basic\n",
    "bart_sample_topkp_evaluator = ExtrinsicEvaluator(bart_sample_topkp_summaries)\n",
    "bart_sample_topkp_evaluator.evaluate('BART SAMPLE TOPKP')\n",
    "bart_sample_topkp_id_to_score = {\n",
    "    \"82d9d60e0360159b6767e0921de88360a93da0f5\": 5,  # This is the best one so far, mentioning that City lost on a Sunday\n",
    "    \"5590b3cff374d453e9bc46e93ea8f90ce94e6c1b\": 5,  # Identical to other models\n",
    "    \"a502426e1671ae4e72690da82f5f2bc9bc589e20\": 5  # Identical to other models\n",
    "}\n",
    "bart_sample_topkp_evaluator.submit_scores(bart_sample_topkp_id_to_score)\n",
    "\n",
    "# Performed slightly better than the best models"
   ],
   "id": "2fa64130608872f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Manual Evaluation Results",
   "id": "c465a91de27a2738"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot ablation models\n",
    "bart_default_evaluator.plot('BART DEFAULT', save_path=os.path.join(results_dir, 'bart_default'))\n",
    "bart_beam_light_evaluator.plot('BART BEAM LIGHT', save_path=os.path.join(results_dir, 'bart_beam_light'))\n",
    "bart_beam_evaluator.plot('BART BEAM', save_path=os.path.join(results_dir, 'bart_beam'))\n",
    "bart_short_evaluator.plot('BART SHORT', save_path=os.path.join(results_dir, 'bart_short'))\n",
    "bart_sample_basic_evaluator.plot('BART SAMPLE BASIC', save_path=os.path.join(results_dir, 'bart_sample_basic'))\n",
    "bart_sample_temp_evaluator.plot('BART SAMPLE TEMP', save_path=os.path.join(results_dir, 'bart_sample_temp'))\n",
    "bart_sample_topk_evaluator.plot('BART SAMPLE TOPK', save_path=os.path.join(results_dir, 'bart_sample_topk'))\n",
    "bart_sample_topp_evaluator.plot('BART SAMPLE TOPP', save_path=os.path.join(results_dir, 'bart_sample_topp'))\n",
    "bart_sample_topkp_evaluator.plot('BART SAMPLE TOPKP', save_path=os.path.join(results_dir, 'bart_sample_topkp'))\n",
    "\n",
    "ablation_avg_manual_scores = {\n",
    "    'bart_default': bart_default_evaluator.avg_score,\n",
    "    'bart_beam_light': bart_beam_light_evaluator.avg_score,\n",
    "    'bart_beam': bart_beam_evaluator.avg_score,\n",
    "    'bart_short': bart_short_evaluator.avg_score,\n",
    "    'bart_sample_basic': bart_sample_basic_evaluator.avg_score,\n",
    "    'bart_sample_temp': bart_sample_temp_evaluator.avg_score,\n",
    "    'bart_sample_topk': bart_sample_topk_evaluator.avg_score,\n",
    "    'bart_sample_topp': bart_sample_topp_evaluator.avg_score,\n",
    "    'bart_sample_topkp': bart_sample_topkp_evaluator.avg_score,\n",
    "    \n",
    "}\n",
    "\n",
    "plot_avg_manual_score(ablation_avg_manual_scores, save_path=os.path.join(results_dir, 'ablation_avg_manual_scores'))"
   ],
   "id": "a0a5b642a93eacbe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
